{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "################################################################################\n",
    "\"\"\"\n",
    "This is a Neural Network Programming Project (NNPP) module for Neurocognitive\n",
    "Methods and Programming (SoSe 2020). This module provides classes for\n",
    "preprocessing and multi-, and binary classification using simple neural network\n",
    "based on the \"Haxby dataset\" <http://data.pymvpa.org/datasets/haxby2001/> with\n",
    ".csv labels. Those are required to run this module:\n",
    "    - python : 3.6.10\n",
    "    - nibabel : 3.1.1\n",
    "    - nipype : 1.6.0 dev0\n",
    "    - nilearn : 0.6.2\n",
    "    - pandas : 1.0.5\n",
    "    - matplotlib : 3.2.2\n",
    "    - tensorflow : 1.13.1\n",
    "    - keras : 2.2.4\n",
    "    - jupyter notebook : 6.0.3\n",
    "And \"You must install FSL to work with the nipype in this module\".\n",
    "\n",
    "This was tested on the docker \"jihoonkim2100/nnpp\"\n",
    "<https://hub.docker.com/r/jihoonkim2100/nnpp> environment\n",
    "which leverage tensorflow and keras based on \"nipype/nipype\":\n",
    "    - docker version : 19.03.1\n",
    "    - docker image OS : Debian GNU/Linux 9\n",
    "    - host OS : Windows 10 Home\n",
    "\n",
    "Belows are the reference for this project:\n",
    "    - scikit-learn https://scikit-learn.org/stable/\n",
    "    - nilearn https://nilearn.github.io/index.html\n",
    "    - nipype https://nipype.readthedocs.io/en/latest/index.html\n",
    "    - keras https://keras.io/\n",
    "\n",
    "Author : JiHoon Kim\n",
    "Last-modified : 13th July 2020\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nibabel as nib\n",
    "import nilearn.image as nimg\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.plotting import plot_anat\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.interfaces.fsl import BET\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam \n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print ( \"--sys.versionâ€”\") \n",
    "print (sys . version)\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(17072020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAcquisiton:\n",
    "    \"\"\"\n",
    "    For data acquistion,\n",
    "    * newfolder : Create new folder.\n",
    "    * inputd : Load the input data.\n",
    "    * outputd : Save the output data.\n",
    "    * reorder : Coordinate RAS to LAS.\n",
    "\n",
    "    Author : JiHoon Kim\n",
    "    Last-modified : 13th July 2020\n",
    "    \"\"\"\n",
    "    def setdata(self,folder,in_path,data,data2,data3,out_path):\n",
    "        \"\"\"\n",
    "        setdata requires a total of seven paramters :\n",
    "        self, folder, in_path, data, data2, data3, and out_path.\n",
    "        \"\"\"\n",
    "        self.folder = folder\n",
    "        self.in_path = in_path\n",
    "        self.data = data\n",
    "        self.data2 = data2\n",
    "        self.data3 = data3\n",
    "        self.out_path = out_path\n",
    "        \n",
    "    def newfolder(self,folder):\n",
    "        \"\"\"\n",
    "        This function for creating a new folder.\n",
    "        folder : read the string, new folder's name.\n",
    "\n",
    "        So that it makes a folder.\n",
    "        \"\"\" \n",
    "        os.mkdir(folder)\n",
    "    \n",
    "    def inputd(self,in_path):\n",
    "        \"\"\"\n",
    "        This function for loading the data.\n",
    "        in_path : read the string, directory address of data.\n",
    "        \n",
    "        Returns a nibabel.nifti1.Nifti1Image.\n",
    "        \"\"\"\n",
    "        print(in_path,\"loaded\")\n",
    "        img = nib.load(in_path)\n",
    "        return img\n",
    "       \n",
    "    def outputd(self,data,out_path):\n",
    "        \"\"\"\n",
    "        This function for saving the data.\n",
    "        data : a nibabel.nifti1.Nifti1Image, data to be saved.\n",
    "        out_path : string, directory address to save.\n",
    "        \"\"\"\n",
    "        nib.save(data,out_path)\n",
    "        print(out_path,\"saved\")\n",
    "        \n",
    "    def reorder(self,data):\n",
    "        \"\"\"\n",
    "        This function reorder LAS to RAS.\n",
    "        data : nibabel.nifti1.Nifti1Image.\n",
    "        \n",
    "        Returns a RAS coordinated nibabel.nifti1.Nifti1Image.\n",
    "        \"\"\"\n",
    "        # Check the coordination and reorder the data LAS to RAS\n",
    "        coord = nib.aff2axcodes(data.affine)\n",
    "        if 'L' == coord[0]:\n",
    "            data = nimg.reorder_img(data,resample=None)\n",
    "            a = f\"Reorder {nib.aff2axcodes(data.affine)}\"\n",
    "            print(a,\"completed\")\n",
    "        else:\n",
    "            a = f\"Original {coord}\"\n",
    "            print(a,\"checked\")\n",
    "        return data\n",
    "  \n",
    "    def acquisition(self,folder,in_path,out_path):\n",
    "        \"\"\"\n",
    "        This function load, reorder, and save the data.\n",
    "        It is dependent on the class name: func.\n",
    "        folder: read the string, new folder's name.\n",
    "        in_path: read the list, element: string, directory address of data.\n",
    "        out_path: read the string, name of the new data.\n",
    "\n",
    "        returns a list, element: string, directory address of new data.\n",
    "        \"\"\"\n",
    "        print('#################_Data_acquisition_started_#################')\n",
    "        acquisition = time.time()\n",
    "\n",
    "        # Check and if not make the directory\n",
    "        if not os.path.isdir(folder):\n",
    "            func.newfolder(folder)\n",
    "\n",
    "        # Load, reorder, and save the data\n",
    "        output_list = []\n",
    "        for i in range(len(in_path)):\n",
    "            output = folder+'/'+out_path[i]\n",
    "            output_list.append(output)\n",
    "            func.outputd(func.reorder(func.inputd(in_path[i])),output)\n",
    "        print(\"acquisition_time :\", \"%.2fs\" %(time.time() - acquisition))\n",
    "        print(\"#################_Data_acquisition_completed_###############\")\n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing(DataAcquisiton):\n",
    "    \"\"\"\n",
    "    For Preprocessing of fMRI Data,\n",
    "    * mask_image : Mask image template.\n",
    "    * mcflirt : Motion correction.\n",
    "    * niftimasker : Smooth, normalize.\n",
    "\n",
    "    Author : JiHoon Kim\n",
    "    Last-modified : 13th July 2020\n",
    "    \"\"\"\n",
    "    def mask_image(self,folder,in_path,out_path):\n",
    "        \"\"\"\n",
    "        This function for creating mask_image.\n",
    "        It is dependent on the class name: func.\n",
    "        folder : read the string, new folder's name.\n",
    "        in_path : read the list, element: string, directory address of data.\n",
    "        out_path : read the string, name of the new data.\n",
    "\n",
    "        For complete details, see the BET() documentation.\n",
    "        <https://nipype.readthedocs.io/en/latest/interfaces.html>\n",
    "        \"\"\"\n",
    "        print('#################_Mask_image_started_#######################')\n",
    "        preprocessing = time.time()\n",
    "\n",
    "        # Check and if not make the directory\n",
    "        if not os.path.isdir(folder):\n",
    "            func.newfolder(folder)\n",
    "\n",
    "        # Create, and save the data : anatomy and whole brain mask image \n",
    "        for i in range(len(in_path)):\n",
    "            output = folder+'/'+out_path[i]\n",
    "            print(in_path[i], \"mask image started\")\n",
    "            skullstrip = BET(in_file=in_path[i],\n",
    "                             out_file=output,\n",
    "                             mask=True)\n",
    "            skullstrip.run()\n",
    "            print(output,\"mask image completed\")\n",
    "        print(\"computation_time :\",\"%.2fs\" %(time.time() - preprocessing))\n",
    "        print('#################_Mask_image_completed_#####################')\n",
    "\n",
    "    def mcflirt(self,folder,in_path):\n",
    "        \"\"\"\n",
    "        This function for motion correction.\n",
    "        It is dependent on the class name: func.\n",
    "        folder : read the string, new folder's name.\n",
    "        in_path : read the list, element: string, directory address of data.\n",
    "\n",
    "        returns a list, element: string, directory address of new data.\n",
    "        For complete details, see the fsl.MCFLIRT() documentation.\n",
    "        <https://nipype.readthedocs.io/en/latest/interfaces.html>\n",
    "        \"\"\" \n",
    "        print('#################_Data_motion_correction_started_###########')\n",
    "        preprocessing = time.time()\n",
    "\n",
    "        # Check and if not make the directory\n",
    "        if not os.path.isdir(folder):\n",
    "            func.newfolder(folder)\n",
    "\n",
    "        # Proceed the motion correction and save the data\n",
    "        output_list = []        \n",
    "        for i in range(len(in_path)):\n",
    "            print(in_path[i],\"motion correction started\")\n",
    "            file = in_path[i].split('/')\n",
    "            output = folder+'/'+file[1]\n",
    "            output_list.append(output)\n",
    "            \n",
    "            mcflt = fsl.MCFLIRT()\n",
    "            mcflt.inputs.in_file = in_path[i]\n",
    "            mcflt.inputs.cost = 'mutualinfo'\n",
    "            mcflt.inputs.out_file = output\n",
    "            mcflt.cmdline\n",
    "            mcflt.run()\n",
    "            print(output,\"motion correction completed\")\n",
    "        print(\"computation_time :\",\"%.2fs\" %(time.time() - preprocessing))\n",
    "        print('#################_Data_motion_correction_completed_#########')\n",
    "        return output_list\n",
    "    \n",
    "    def niftimasker(self,in_path,out_path):\n",
    "        \"\"\"\n",
    "        This function for masking whole brain image.\n",
    "        in_path : read the list, element: string, directory address of data.\n",
    "        out_path : read the list, element: string, directory address of data.\n",
    "        For complete details, see the nilearn.input_data.NiftiMasker.\n",
    "        <https://nilearn.github.io/modules/reference.html>\n",
    "        \"\"\"\n",
    "        print('#################_Data_preprocessing_started_###############')\n",
    "        preprocessing = time.time()\n",
    "\n",
    "        # Smoothing fwhm : 12, mask, standardize, detrend, and resampling.\n",
    "        for i in range(len(in_path)):\n",
    "            print(in_path[i])\n",
    "            print(\"smoothing_fwhm : 12, and normalization started\")\n",
    "            masker = NiftiMasker(mask_img=out_path[i],smoothing_fwhm= 12,\n",
    "                                 standardize=True,detrend=True,\n",
    "                                 mask_strategy='template')\n",
    "            func_data = masker.fit_transform(in_path[i])\n",
    "            print(in_path[i],'to func_data')\n",
    "            print('smoothing_fwhm : 12, and normalization completed')\n",
    "        print(\"computation_time :\", \"%.2fs\" %(time.time() - preprocessing))\n",
    "        print('#################_Data_preprocessing_completed_#############')\n",
    "        return func_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(DataPreprocessing):\n",
    "    \"\"\"\n",
    "    For Nueral Network Analysis,\n",
    "    * categorical_label : Load the categorical label.\n",
    "    * categorical_dataset : Prepare the dataset, train and test set.\n",
    "    * grid_search : Proceed the gridsearch on the train set.\n",
    "    * cross_validation : Do the cross validation on the train set.\n",
    "    * test_model : Train, test, and save the model on the dataset.\n",
    "\n",
    "    Author : JiHoon Kim\n",
    "    Last-modified : 13th July 2020 \n",
    "    \"\"\"\n",
    "    def categorical_label(self,in_path):\n",
    "        \"\"\"\n",
    "        This function load the multiclass label of the dataset.\n",
    "        in_path : read the string, directory address of the label data.\n",
    "        \n",
    "        return two lists (numbers_list, labels_list which contain labels and\n",
    "        sample order number of fMRI data), elements are integer.\n",
    "        \"\"\"\n",
    "        print('#################_Categorical_label_checked_################')\n",
    "        preprocessing = time.time()\n",
    "\n",
    "        # Load the label, .csv file\n",
    "        labels = pd.read_csv(in_path)\n",
    "        stimuli = labels['label']\n",
    "\n",
    "        # Select the stimuli labels,\n",
    "        # 0 : 'scissors', 1 :'face', 2 : 'cat', 3 : 'shoe', 4 : 'house',\n",
    "        # 5 : 'scrambledpix', 6 : 'bottle', and 7 : 'chair'.\n",
    "        numbers_list = []\n",
    "        labels_list = []\n",
    "        num = 0\n",
    "        for i in stimuli: \n",
    "            num += 1\n",
    "            if i == 'scissors':\n",
    "                numbers_list.append(num)\n",
    "                labels_list.append(0)\n",
    "            elif i == 'face':\n",
    "                numbers_list.append(num)\n",
    "                labels_list.append(1)\n",
    "            elif i == 'cat':  \n",
    "                numbers_list.append(num)\n",
    "                labels_list.append(2)\n",
    "            elif i == 'shoe':\n",
    "                numbers_list.append(num)\n",
    "                labels_list.append(3)\n",
    "            elif i == 'house':\n",
    "                numbers_list.append(num)\n",
    "                labels_list.append(4)\n",
    "            elif i == 'scrambledpix':\n",
    "                numbers_list.append(num)\n",
    "                labels_list.append(5)\n",
    "            elif i == 'bottle':\n",
    "                numbers_list.append(num)\n",
    "                labels_list.append(6)\n",
    "            elif i == 'chair':\n",
    "                numbers_list.append(num)\n",
    "                labels_list.append(7)\n",
    "        print('labels_list:',len(labels_list),'loaded')\n",
    "        print(\"computation_time :\",\"%.2fs\" %(time.time() - preprocessing))\n",
    "        print('#################_Categorical_label_loaded_#################')\n",
    "        return numbers_list, labels_list\n",
    "\n",
    "    def categorical_dataset(self,in_path,data,data2):\n",
    "        \"\"\"\n",
    "        This function prepare the dataset,\n",
    "        in_path : read a list which contains labels data.\n",
    "        data : read a np.array which contains feature data.\n",
    "        data2 : read a list which contains fMRI orders data.\n",
    "\n",
    "        return four np.array, and one integer,\n",
    "        train_data, test_data, train_labels, test_labels, and input size. \n",
    "        \"\"\"\n",
    "        print('#################_Categorical_feature_checked_##############')\n",
    "        preprocessing = time.time()\n",
    "\n",
    "        # Collect only labels stimuli fMRI data\n",
    "        func_data = []\n",
    "        labels_list = in_path\n",
    "        for i in data2:\n",
    "            func_data.append(data[i])\n",
    "        \n",
    "        # Create train set and test set with 5:1 ratio\n",
    "        fold = int(len(func_data)*5/6)\n",
    "        train_data = np.array(func_data[:fold])\n",
    "        test_data = np.array(func_data[fold:])\n",
    "\n",
    "        train_labels = np.array(labels_list[:fold])\n",
    "        test_labels = np.array(labels_list[fold:])\n",
    "        print(\"train_data, test_data :\",len(train_data),len(test_data))\n",
    "\n",
    "        # Normalize the dataset\n",
    "        mean = train_data.mean(axis=0)\n",
    "        train_data -= mean\n",
    "        std = train_data.std(axis=0)\n",
    "        train_data /= std\n",
    "\n",
    "        test_data -= mean\n",
    "        test_data /= std\n",
    "        \n",
    "        train_labels = to_categorical(train_labels)\n",
    "        test_labels = to_categorical(test_labels)    \n",
    "        \n",
    "        size = len(train_data[1])\n",
    "        print(\"Dataset standardization completed\")\n",
    "        print(\"size:\",len(train_data[1]),\" train_data:\",len(train_data),\n",
    "              \"and\",\"test_data:\",len(test_data),\"loaded\")\n",
    "        print(\"computation_time :\",\"%.2fs\" %(time.time() - preprocessing))\n",
    "        print('#################_Categorical_feature_loaded_###############')\n",
    "        return train_data,test_data,train_labels,test_labels,size \n",
    "    \n",
    "    def grid_search(self,data,data2):\n",
    "        \"\"\"\n",
    "        This function for grid search,\n",
    "        data : read a np.array which contains train_data.\n",
    "        data2 : read a list which contains train_labels.\n",
    "        \"\"\"\n",
    "        print('#################_Grid_search_started_######################')\n",
    "        preprocessing = time.time()\n",
    "\n",
    "        def gridsearch_model(dropout_rate,neurons,neurons2):\n",
    "            \"\"\"\n",
    "            This function for building a gridsearch_model,\n",
    "            dropout_rate : float, 0 to 1 normally 0.1, 0.3, 0.5 etc.\n",
    "            neurons : integer, first dropout layer parameter.\n",
    "            neurons2 : integer, second dropout layer parameter.\n",
    "\n",
    "            returns a model.\n",
    "            \"\"\"  \n",
    "            model = models.Sequential()\n",
    "            model.add(Dense(neurons,\n",
    "                            input_shape = (size, ),\n",
    "                            activation = 'relu'))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(neurons2,\n",
    "                            activation = 'relu'))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(8,\n",
    "                            activation = 'softmax'))   \n",
    "    \n",
    "            model.compile(optimizer = 'Adam',\n",
    "                          loss = 'categorical_crossentropy',\n",
    "                          metrics = ['acc'])\n",
    "            return model\n",
    "\n",
    "        model = KerasClassifier(build_fn = gridsearch_model,\n",
    "                                batch_size = 50, epochs = 100)\n",
    "\n",
    "        # Gridsearch on hyper parameters\n",
    "        dropout_rate = [0.1, 0.3, 0.5]\n",
    "        neurons = [20, 25]\n",
    "        neurons2 = [10, 15]\n",
    "        epochs = [50, 100]\n",
    "        batch_size = [25, 50]\n",
    "\n",
    "        param_grid = dict(dropout_rate = dropout_rate,\n",
    "                          epochs = epochs, batch_size = batch_size,\n",
    "                          neurons = neurons,neurons2 = neurons2)\n",
    "        grid = GridSearchCV(estimator = model, param_grid = param_grid,\n",
    "                            n_jobs=-1)\n",
    "        grid_result = grid.fit(data,data2)\n",
    "\n",
    "        # Summarize results\n",
    "        means = grid_result.cv_results_['mean_test_score']\n",
    "        stds = grid_result.cv_results_['std_test_score']\n",
    "        params = grid_result.cv_results_['params']\n",
    "        for mean,stdev,param in zip(means,stds,params):\n",
    "            print(\"%f (%f) with: %r\" % (mean,stdev,param))\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_,\n",
    "                                     grid_result.best_params_))\n",
    "        print(\"computation_time :\",\"%.2fs\" %(time.time() - preprocessing))\n",
    "        print('#################_Grid_search_completed_####################')              \n",
    "       \n",
    "    def cross_validation(self,in_path,data,data2,data3):\n",
    "        \"\"\"\n",
    "        This function for cross validation,\n",
    "        in_path : read a list, which contains 0 : integer 3, 4, 5 (K-fold),\n",
    "        1 : integer 25, 32 (batch_size), 2 : float 0.1 to 1.0 (drop_out_rate),\n",
    "        3 : integer 50, 100 (num_epochs), 4 : integer 10, 20 (neuron), and\n",
    "        5 : integer 20, 30 (neurons2).\n",
    "        data : 2d np array, train_data.\n",
    "        data2 : 2d np array, train_labels.\n",
    "        \"\"\"\n",
    "        print('#################_Cross_validation_checked_#################')\n",
    "        preprocessing = time.time()\n",
    "\n",
    "        def cross_validation_model(drop_out,neuron,neuron2):\n",
    "            \"\"\"\n",
    "            This function for building a cross_validation_model,\n",
    "            dropout_rate : float, 0 to 1 normally 0.1, 0.3, 0.5 etc.\n",
    "            neurons : integer, first dropout layer parameter.\n",
    "            neurons2 : integer, second dropout layer parameter.\n",
    "\n",
    "            returns a model\n",
    "            \"\"\"\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(neuron,activation = 'relu',\n",
    "                                   input_shape = (size, )))\n",
    "            model.add(layers.Dropout(drop_out))\n",
    "            model.add(layers.Dense(neuron2,activation = 'relu'))\n",
    "            model.add(layers.Dropout(drop_out))\n",
    "            model.add(layers.Dense(8,activation = 'softmax'))\n",
    "            model.compile(optimizer = 'Adam',\n",
    "                          loss = 'categorical_crossentropy',\n",
    "                          metrics = ['acc'])\n",
    "            return model \n",
    "        \n",
    "        train_data = data2\n",
    "        train_labels = data3\n",
    "        \n",
    "        k = 5\n",
    "        num_val_samples = len(train_data)//k\n",
    "        num_epochs = 200\n",
    "        all_acc_histories = []\n",
    "        all_loss_histories = []\n",
    "        all_val_loss_histories = []\n",
    "        all_val_acc_histories = []\n",
    "\n",
    "        # Cross validation check, train set and validation set with 4:1 ratio\n",
    "        for i in range(k):\n",
    "            print('current fold #', i+1)\n",
    "            val_data = train_data[i*num_val_samples: (i+1)*num_val_samples]\n",
    "            val_labels = train_labels[i*num_val_samples: (i+1)*num_val_samples]\n",
    "    \n",
    "            partial_train_data = np.concatenate(\n",
    "                [train_data[:i*num_val_samples],\n",
    "                 train_data[(i+1)*num_val_samples:]],\n",
    "                axis=0)\n",
    "            partial_train_labels = np.concatenate(\n",
    "                [train_labels[:i*num_val_samples],\n",
    "                 train_labels[(i+1)*num_val_samples:]],\n",
    "                axis=0)\n",
    "    \n",
    "            model = cross_validation_model(data[0],data[1],data[2])\n",
    "            history = model.fit(partial_train_data,partial_train_labels,\n",
    "                                validation_data=(val_data,val_labels),\n",
    "                                epochs = num_epochs,batch_size = in_path,\n",
    "                                verbose = 0)\n",
    "\n",
    "            # Shows the acc and loss plot\n",
    "            fig, loss_ax = plt.subplots()\n",
    "            acc_ax = loss_ax.twinx()\n",
    "\n",
    "            plt.title('Training and validation acc and loss')\n",
    "            loss_ax.plot(history.history['loss'],'y',label = 'train loss')\n",
    "            loss_ax.plot(history.history['val_loss'],'r',label = 'val loss')\n",
    "\n",
    "            acc_ax.plot(history.history['acc'],'b',label = 'train acc')\n",
    "            acc_ax.plot(history.history['val_acc'],'g',label = 'val acc')\n",
    "\n",
    "            loss_ax.set_xlabel('epoch')\n",
    "            loss_ax.set_ylabel('loss')\n",
    "            acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "            loss_ax.legend(loc = 'upper left')\n",
    "            acc_ax.legend(loc = 'lower left')\n",
    "            plt.show()\n",
    "\n",
    "            # Save the histories: acc, loss, val_acc, and val_loss\n",
    "            all_acc_histories.append(history.history['acc'])\n",
    "            all_loss_histories.append(history.history['loss'])\n",
    "            all_val_acc_histories.append(history.history['val_acc'])\n",
    "            all_val_loss_histories.append(history.history['val_loss'])\n",
    "\n",
    "        # Compute the average histories\n",
    "        average_acc_history = [\n",
    "            np.mean([x[i] for x in all_acc_histories])\n",
    "            for i in range(num_epochs)]\n",
    "        average_loss_history = [\n",
    "            np.mean([x[i] for x in all_loss_histories]) \n",
    "            for i in range(num_epochs)]\n",
    "        average_val_acc_history = [\n",
    "            np.mean([x[i] for x in all_val_acc_histories]) \n",
    "            for i in range(num_epochs)]\n",
    "        average_val_loss_history = [\n",
    "            np.mean([x[i] for x in all_val_loss_histories]) \n",
    "            for i in range(num_epochs)]\n",
    "\n",
    "        # Shows the average acc and loss plot\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        acc_ax = loss_ax.twinx()\n",
    "        plt.title('Average training and validation acc and loss')\n",
    "        loss_ax.plot(average_loss_history,'y',label = 'train loss')\n",
    "        loss_ax.plot(average_val_loss_history,'r',label = 'val loss')\n",
    "        acc_ax.plot(average_acc_history,'b',label = 'train acc')\n",
    "        acc_ax.plot(average_val_acc_history,'g',label = 'val acc')\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        acc_ax.set_ylabel('accuracy')\n",
    "        loss_ax.legend(loc = 'upper left')\n",
    "        acc_ax.legend(loc = 'lower left')\n",
    "        plt.show()\n",
    "\n",
    "        # Shows the average loss plot\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        plt.title('Average training and validation loss')\n",
    "        loss_ax.plot(average_loss_history,'y',label = 'train loss')\n",
    "        loss_ax.plot(average_val_loss_history,'r',label = 'val loss')\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        loss_ax.legend(loc = 'upper left')\n",
    "        plt.show()\n",
    "\n",
    "        # Shows the average acc plot\n",
    "        fig, acc_ax = plt.subplots()\n",
    "        plt.title('Average training and validation acc')\n",
    "        acc_ax.plot(average_acc_history,'b',label = 'train acc')\n",
    "        acc_ax.plot(average_val_acc_history,'g',label = 'val acc')\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        acc_ax.set_ylabel('accuracy')\n",
    "        acc_ax.legend(loc = 'lower right')\n",
    "        plt.show()\n",
    "        print(\"computation_time :\",\"%.2fs\" %(time.time() - preprocessing))\n",
    "        print('#################_Cross_validation_completed_###############')\n",
    "        \n",
    "    def test_model(self,data,data2):\n",
    "        \"\"\"\n",
    "        This function for train, test, and save the model,\n",
    "        data : read a integer, num_epochs.\n",
    "        data2 : read a list which 0 : integer (batch_size),\n",
    "        1 : float (drop_out_rate), 2 : integer (neuron), 3: integer (neuron2).\n",
    "        \"\"\"    \n",
    "        print('#################_Test_model_started_#######################')\n",
    "        preprocessing = time.time()\n",
    "        \n",
    "        def build_model(drop_out,neuron,neuron2):\n",
    "            \"\"\"\n",
    "            This function for building a model,\n",
    "            dropout_rate : float, 0 to 1 normally 0.1, 0.3, 0.5 etc.\n",
    "            neurons : integer, first dropout layer parameter.\n",
    "            neurons2 : integer, second dropout layer parameter.\n",
    "\n",
    "            returns a model.\n",
    "            \"\"\" \n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(neuron,activation = 'relu',\n",
    "                                   input_shape = (size, )))\n",
    "            model.add(layers.Dropout(drop_out))\n",
    "            model.add(layers.Dense(neuron2,activation = 'relu'))\n",
    "            model.add(layers.Dropout(drop_out))\n",
    "            model.add(layers.Dense(8,activation = 'softmax'))\n",
    "            model.compile(optimizer = 'Adam',loss = 'categorical_crossentropy',\n",
    "                          metrics = ['acc'])\n",
    "            return model\n",
    "\n",
    "        model = build_model(data2[0],data2[1],data2[2])\n",
    "        print(f\"-- Multiclassification_model, epoch: {data} --\")\n",
    "        model.summary()\n",
    "\n",
    "        # Train a model\n",
    "        model.fit(train_data,train_labels,epochs=data,\n",
    "                  batch_size = int(batch_size), verbose = 0)\n",
    "        print(\"-- Evaluate --\")\n",
    "\n",
    "        # Test a model\n",
    "        score = model.evaluate(test_data,test_labels,verbose = 1)\n",
    "        print('loss :',score[0])\n",
    "        print(\"%s : %.2f%%\" %(model.metrics_names[1],score[1]*100))\n",
    "\n",
    "        # Save a model\n",
    "        model.save(f\"model_epoch:{data}_weight.h5\")\n",
    "        with open(f\"model_epoch:{data}_architecture.json\",'w') as f:\n",
    "            f.write(model.to_json())\n",
    "        print(f\"model_epoch : {data}, weight_and_architecture_saved\")\n",
    "        print(\"computation_time :\",\"%.2fs\" %(time.time() - preprocessing))\n",
    "        print('#################_Test_model_completed_#####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extra(NN):\n",
    "    \"\"\"\n",
    "    Extra module\n",
    "    * anat_mask : Show the anatomy image and whole brain mask image.\n",
    "    * func_img : Display the fMRI signal, original and motion correction one.\n",
    "    * binary_classification : Run face vs. house binary classification.\n",
    "\n",
    "    Author : JiHoon Kim\n",
    "    Last-modified : 13th July 2020\n",
    "    \"\"\"\n",
    "    def anat_mask(self,in_path,out_path):\n",
    "        \"\"\"\n",
    "        This function for showing anat_mask.\n",
    "        in_path : read the list, element: string, directory address of image.\n",
    "        out_path : read the list, element: string, for title of the image.\n",
    "        \"\"\"\n",
    "        for i in range(len(in_path)):\n",
    "            plot_anat(in_path[i],title = out_path[i],\n",
    "                      display_mode = 'ortho',dim = -1,\n",
    "                      draw_cross = False, annotate = False)\n",
    "            \n",
    "    def func_img(self,in_path,data,data2,out_path):\n",
    "        \"\"\"\n",
    "        This function for displaying func_img.\n",
    "        in_path : read the string, directory address of original_image.\n",
    "        data : read the string, for title of original_image.\n",
    "        data2 : read the string, for title of other_image.\n",
    "        out_path : read the string, directory address of other_image.\n",
    "        \"\"\"\n",
    "        # Load the images\n",
    "        rod = func.inputd(in_path)\n",
    "        prp = func.inputd(out_path)\n",
    "                         \n",
    "        # Plot a representative voxel\n",
    "        x,y,z = 31,35,42\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.plot(rod.get_data()[x,y,z,:])\n",
    "        plt.plot(prp.get_data()[x,y,z,:])\n",
    "        plt.legend([data,data2])\n",
    "        \n",
    "    def binary_classification(self,data,data2,data3):\n",
    "        \"\"\"\n",
    "        This function for binary classification.\n",
    "        data : read the 2d np array, fMRI dataset.\n",
    "        data2 : read the 2d np array, labels list.\n",
    "        data3 : read the 2d np array, fMRI order number list.\n",
    "        \"\"\"\n",
    "        print('#################_Binary_classification_started_############')\n",
    "        preprocessing = time.time()\n",
    "\n",
    "        func_data = data\n",
    "        labels_list =data2\n",
    "        numbers_list = data3\n",
    "        binary_label_list = []\n",
    "        binary_data = []\n",
    "\n",
    "        # Collect only face and house labels stimuli and fMRI data\n",
    "        for i in range(len(labels_list)):\n",
    "            if labels_list[i] == 1:\n",
    "                binary_data.append(func_data[numbers_list[i]])\n",
    "                binary_label_list.append(0) # face stimuli\n",
    "            elif labels_list[i] == 4:\n",
    "                binary_data.append(func_data[numbers_list[i]])\n",
    "                binary_label_list.append(1) # house stimuli\n",
    "\n",
    "        # Create train set and test set with 5:1 ratio\n",
    "        fold = int(len(binary_data)*5/6)\n",
    "        train_data = np.array(binary_data[:fold])\n",
    "        test_data = np.array(binary_data[fold:])\n",
    "        train_labels = binary_label_list[:fold]\n",
    "        test_labels = binary_label_list[fold:]\n",
    "\n",
    "        # Normalize the dataset\n",
    "        mean = train_data.mean(axis=0)\n",
    "        train_data -= mean\n",
    "        std = train_data.std(axis=0)\n",
    "        train_data /= std\n",
    "\n",
    "        test_data -= mean\n",
    "        test_data /= std\n",
    "\n",
    "        # Create train set and validation set with 4:1 ratio\n",
    "        fold2 = int(len(train_data)*1/5)\n",
    "        binary_partial_train_data = train_data[fold2:]\n",
    "        binary_val_data = train_data[:fold2]\n",
    "        binary_partial_train_labels = train_labels[fold2:]\n",
    "        binary_val_labels = train_labels[:fold2]\n",
    "        print(\"train_data:\",len(train_data),\" test_data :\",len(test_data))\n",
    "        print(\"partial_train_data:\",len(binary_partial_train_data),\n",
    "              \" val_data:\",len(binary_val_data))\n",
    "        print(\"Dataset standardization completed\")\n",
    "        print(\"size:\",len(train_data[1]),\" train_data:\",len(train_data),\n",
    "              \"and\",\"test_data:\",len(test_data),\"loaded\")\n",
    "    \n",
    "        # Build the binary classification model\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(25,activation = 'relu',\n",
    "                               input_shape = (len(train_data[1]), )))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.Dense(15,activation = 'relu'))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.Dense(1,activation = 'sigmoid'))\n",
    "        model.compile(optimizer = 'Adam',\n",
    "                      loss = 'binary_crossentropy',\n",
    "                      metrics = ['acc'])\n",
    "        model.summary()\n",
    "\n",
    "        # Train a model\n",
    "        history = model.fit(binary_partial_train_data,\n",
    "                            binary_partial_train_labels,\n",
    "                            validation_data = (binary_val_data,\n",
    "                                               binary_val_labels),\n",
    "                            epochs = 10, batch_size = 18, verbose = 1)\n",
    "\n",
    "        # Shows the acc and loss plot\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        acc_ax = loss_ax.twinx()\n",
    "\n",
    "        plt.title('Training and validation acc and loss')\n",
    "        loss_ax.plot(history.history['loss'],'y',label = 'train loss')\n",
    "        loss_ax.plot(history.history['val_loss'],'r',label = 'val loss')\n",
    "\n",
    "        acc_ax.plot(history.history['acc'],'b',label = 'train acc')\n",
    "        acc_ax.plot(history.history['val_acc'],'g',label = 'val acc')\n",
    "\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "        loss_ax.legend(loc = 'upper left')\n",
    "        acc_ax.legend(loc = 'lower left')\n",
    "        plt.show()\n",
    "\n",
    "        # Test a model\n",
    "        print(\"-- Evaluate --\")\n",
    "        score = model.evaluate(test_data,test_labels,verbose=1)\n",
    "        print('loss :', score[0])\n",
    "        print(\"%s : %.2f%%\" %(model.metrics_names[1],score[1]*100))\n",
    "        print(\"computation_time :\",\"%.2fs\" %(time.time() - preprocessing))\n",
    "        print('#################_Binary_classification_completed_##########')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interface(Extra):\n",
    "    \"\"\"\n",
    "    Interface module\n",
    "    * preprocessing_interface : Set the initial data input points.\n",
    "    * hyperparameter_interface : Tuning the model parameters.\n",
    "    * extra_image_interface : Run the extra modules.\n",
    "\n",
    "    Author : JiHoon Kim\n",
    "    Last-modified : 13th July 2020\n",
    "    \"\"\"\n",
    "    def preprocessing_interface(self,data):\n",
    "        \"\"\"\n",
    "        This function for setting the input data,\n",
    "        preprocessing the feature, and loading the labels.\n",
    "        data : 'Y' use the default subject-3.\n",
    "        \"\"\"\n",
    "        folder1 = 'acquisition'\n",
    "        folder2 = 'preprocessing'\n",
    "\n",
    "        # Use the defualt: subject-3 data\n",
    "        if 'Y'==data:\n",
    "            input_file = ['ds000105/sub-3/func/bold.nii.gz']\n",
    "            output_name = ['sub-3_bold.nii.gz']\n",
    "            mask_file = ['ds000105/sub-3/mask/mask4_vt.nii.gz']\n",
    "            input_csv = 'ds000105/sub-3/task_events.csv'\n",
    "\n",
    "        # Select the subject nubmer between 1 to 6.\n",
    "        else:\n",
    "            number = input(\"Please type new subject number betweeen 1 to 6: \")\n",
    "            input_file = ['ds000105/sub-'+str(number)+'/func/bold.nii.gz']\n",
    "            output_name = ['sub-'+str(number)+'_bold.nii.gz']\n",
    "            mask_file = ['ds000105/sub-'+str(number)+'/mask/mask4_vt.nii.gz']\n",
    "            input_csv = 'ds000105/sub-'+str(number)+'/task_events.csv'\n",
    "\n",
    "        # Proceed the preprocessing features and labels\n",
    "        prep_file = func.acquisition(folder1,input_file,output_name)\n",
    "        mcflirt_file = func.mcflirt(folder2,prep_file)\n",
    "        func_data = func.niftimasker(mcflirt_file,mask_file)\n",
    "        numbers_list,labels_list = func.categorical_label(input_csv)\n",
    "        return func_data,numbers_list,labels_list\n",
    "        \n",
    "    def hyperparameter_interface(self):\n",
    "        \"\"\"\n",
    "        This function for tuning the hpyerparameter data.\n",
    "        \"\"\"\n",
    "\n",
    "        # set the hyperparameter\n",
    "        do_rate = input(\"Please type the drop_out rate : \")\n",
    "        n1 = input(\"Please type the neuron layers 1 : \")\n",
    "        n2 = input(\"Please type the neuron layers 2 : \")\n",
    "        cv_parameter = [float(do_rate),int(n1),int(n2)]\n",
    "        return cv_parameter\n",
    "  \n",
    "    def extra_image_interface(self,data):\n",
    "        \"\"\"\n",
    "        This function for setting the input data,\n",
    "        creating a mask image, displaying images.\n",
    "        data : 'Y' use the default subject-3.\n",
    "        \"\"\"\n",
    "        folder3 = 'mask'\n",
    "\n",
    "        # Use the defualt: subject-3 data\n",
    "        if 'Y'==data:\n",
    "            mask_in_path = ['ds000105/sub-3/anat/anat.nii.gz']\n",
    "            mask_out_path = ['sub-3_anat']\n",
    "            image = ['ds000105/sub-3/anat/anat.nii.gz',\n",
    "                     'mask/sub-3_anat.nii.gz','mask/sub-3_anat_mask.nii.gz',\n",
    "                     'ds000105/sub-3/mask/mask4_vt.nii.gz']\n",
    "            title = ['sub-3_anat','sub-3_WM','sub-3_mask','sub-3_vt']\n",
    "            func.func_img('acquisition/sub-3_bold.nii.gz','original',\n",
    "                          'motion correction','preprocessing/sub-3_bold.nii.gz')\n",
    "\n",
    "        # Select the subject nubmer between 1 to 6.\n",
    "        else:\n",
    "            number2 = input(\"Please type the subject number betweeen 1 to 6: \")\n",
    "            mask_in_path = ['ds000105/sub-'+str(number2)+'/anat/anat.nii.gz']\n",
    "            mask_out_path = ['sub-'+str(number2)+'_anat']\n",
    "            image = ['ds000105/sub-'+str(number2)+'/anat/anat.nii.gz',\n",
    "                     'mask/sub-'+str(number2)+'_anat.nii.gz',\n",
    "                     'mask/sub-'+str(number2)+'_anat_mask.nii.gz',\n",
    "                     'ds000105/sub-'+str(number2)+'/mask/mask4_vt.nii.gz']\n",
    "            title = ['sub-'+str(number2)+'_anat','sub-'+str(number2)+'_WM',\n",
    "                     'sub-'+str(number2)+'_mask','sub-'+str(number2)+'_vt']\n",
    "            func.func_img('acquisition/sub-'+str(number2)+'_bold.nii.gz',\n",
    "                          'original','motion correction',\n",
    "                          'preprocessing/sub-'+str(number2)+'_bold.nii.gz')                     \n",
    "        func.mask_image(folder3,mask_in_path,mask_out_path)\n",
    "        func.anat_mask(image,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##INSTRUCTIONS_STARTED##########################################################\n",
    "print('Welcome to neural network programming project interface')\n",
    "func = Interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MAIN_ANALYSIS:PREPROCESSING_TO_GRIDSEARCH#####################################\n",
    "print('#################_Analysis_started_#########################')\n",
    "programming = time.time()\n",
    "\n",
    "# Choose the default or other subject between 1 to 6\n",
    "choice = input(\"Do you want to analysis Subject 3 or not? Type Y or N : \")\n",
    "\n",
    "# Preprocess the feature and load the list\n",
    "func_data,numbers_list,labels_list = func.preprocessing_interface(choice)\n",
    "\n",
    "# Prepare the dataset\n",
    "train_data,test_data,train_labels,test_labels,size = func.categorical_dataset(\n",
    "    labels_list,func_data,numbers_list)\n",
    "\n",
    "# Run the gridsearch\n",
    "func.grid_search(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MAIN_ANALYSIS:CROSS_VALIDATION_TO_TRAIN_AND_TEST##############################\n",
    "# Run the cross validation k = 5\n",
    "batch_size = input(\"Please type the batch size : \")\n",
    "cv_parameter = func.hyperparameter_interface()\n",
    "func.cross_validation(int(batch_size),cv_parameter,train_data,train_labels)\n",
    "\n",
    "# Train and test the model\n",
    "for final_epochs in [25,50,75,100,200]:\n",
    "    func.test_model(final_epochs,cv_parameter)\n",
    "print(\"total_computation_time :\",\"%.2fs\" %(time.time() - programming))\n",
    "print('#################_Analysis_completed_#######################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EXTRA_ANALYSIS:MASK_AND_SHOW_IMAGES###########################################\n",
    "print('#################_Analysis_extra_started####################')\n",
    "programming2 = time.time()\n",
    "\n",
    "# Choose the default or other subject between 1 to 6 based on previous analysis\n",
    "choice = input(\"Did you test subject 3 or not? Type Y or N : \")\n",
    "\n",
    "# Mask the image and shows the images: fMRI data signal, and mask iamge\n",
    "func.extra_image_interface(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EXTRA_ANALYSIS:BINARY_CLASSIFICATION##########################################\n",
    "# Do the binary classification and shows the result with model\n",
    "func.binary_classification(func_data,labels_list,numbers_list)\n",
    "print(\"total_computation_time :\",\"%.2fs\" %(time.time() - programming2))\n",
    "print('#################_Analysis_extra_completed##################')\n",
    "##INSTRUCTIONS_COMPLETED########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
